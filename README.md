# Learning-PySpark
Saving Learning progress of PySpark Google colab notebook

# ğŸš€ Learning PySpark

This repository documents my hands-on journey learning **PySpark**, the Python API for Apache Spark. As a Data Science and Engineering enthusiast, I'm building this resource to master big data processing and distributed computing.

---

## ğŸ§  Why PySpark?

In todayâ€™s data-driven world, traditional data analysis tools struggle with large-scale datasets. **PySpark** solves that by offering:

- âš¡ Speed and performance for big data
- ğŸ§© Scalable data transformation pipelines
- ğŸ’» Easy integration with Python and the Spark ecosystem
- ğŸ“ˆ Real-time and batch data processing capabilities

---

## ğŸ§¾ Repository Structure

| Folder/File                | Description |
|---------------------------|-------------|
| `notebooks/`              | Jupyter notebooks with PySpark code |
| `datasets/`               | Sample datasets used during exercises |
| `projects/`               | Mini-projects using real-world datasets |
| `README.md`               | You are here! |
| `requirements.txt`        | Python + PySpark environment setup |

---

## ğŸ“˜ What I'm Learning

âœ… Basics of PySpark  
âœ… SparkSession, RDDs, and DataFrames  
âœ… Data loading and schema inference  
âœ… Transformations vs Actions  
âœ… Data wrangling and filtering  
âœ… Working with missing data  
âœ… Aggregations and group operations  
âœ… Joins and complex queries  
âœ… Window functions  
âœ… Spark SQL  
âœ… Optimization with caching and partitioning  
âœ… Real-world mini-projects

---

## ğŸ“Š Sample Topics Covered (Notebook Titles)

- `01_spark_setup_and_intro.ipynb`
- `02_dataframe_operations.ipynb`
- `03_handling_missing_data.ipynb`
- `04_groupby_and_aggregations.ipynb`
- `05_joining_dataframes.ipynb`
- `06_spark_sql_and_views.ipynb`
- `07_window_functions.ipynb`
- `08_project_nyc_taxi_analysis.ipynb`

---

## ğŸŒ Environment Setup

To run these notebooks:

1. Install Java (JDK 11)
2. Install PySpark:
   ```bash
   pip install pyspark
