# Learning-PySpark
Saving Learning progress of PySpark Google colab notebook

# 🚀 Learning PySpark

This repository documents my hands-on journey learning **PySpark**, the Python API for Apache Spark. As a Data Science and Engineering enthusiast, I'm building this resource to master big data processing and distributed computing.

---

## 🧠 Why PySpark?

In today’s data-driven world, traditional data analysis tools struggle with large-scale datasets. **PySpark** solves that by offering:

- ⚡ Speed and performance for big data
- 🧩 Scalable data transformation pipelines
- 💻 Easy integration with Python and the Spark ecosystem
- 📈 Real-time and batch data processing capabilities

---

## 🧾 Repository Structure

| Folder/File                | Description |
|---------------------------|-------------|
| `notebooks/`              | Jupyter notebooks with PySpark code |
| `datasets/`               | Sample datasets used during exercises |
| `projects/`               | Mini-projects using real-world datasets |
| `README.md`               | You are here! |
| `requirements.txt`        | Python + PySpark environment setup |

---

## 📘 What I'm Learning

✅ Basics of PySpark  
✅ SparkSession, RDDs, and DataFrames  
✅ Data loading and schema inference  
✅ Transformations vs Actions  
✅ Data wrangling and filtering  
✅ Working with missing data  
✅ Aggregations and group operations  
✅ Joins and complex queries  
✅ Window functions  
✅ Spark SQL  
✅ Optimization with caching and partitioning  
✅ Real-world mini-projects

---

## 📊 Sample Topics Covered (Notebook Titles)

- `01_spark_setup_and_intro.ipynb`
- `02_dataframe_operations.ipynb`
- `03_handling_missing_data.ipynb`
- `04_groupby_and_aggregations.ipynb`
- `05_joining_dataframes.ipynb`
- `06_spark_sql_and_views.ipynb`
- `07_window_functions.ipynb`
- `08_project_nyc_taxi_analysis.ipynb`

---

## 🌐 Environment Setup

To run these notebooks:

1. Install Java (JDK 11)
2. Install PySpark:
   ```bash
   pip install pyspark
