{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkj57WmSqK90B9vY5B6hLa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrit6878/Learning-PySpark/blob/main/ReadJsonFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading or uploading the json file to pyspark -\n",
        "\n",
        "everthing is similar to uploading csv files just here by default multiline=false so if you records are written in multiline format of json type file you have to specify multiLine = True eg:\n",
        "\n",
        "`df = spark.read.json(\"file_path\" , multiLine = True)`\n",
        "\n",
        "otherwise uploading json will throw an error showing \"corrupt record\"\n"
      ],
      "metadata": {
        "id": "b1Kx07zeJx6N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eXzfEz5HP-3",
        "outputId": "a2a22f1e-0dbf-4e40-ff47-7479d43ce771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Series: string (nullable = true)\n",
            " |-- X: double (nullable = true)\n",
            " |-- Y: double (nullable = true)\n",
            "\n",
            "+------+----+-----+\n",
            "|Series|   X|    Y|\n",
            "+------+----+-----+\n",
            "|     I|10.0| 8.04|\n",
            "|     I| 8.0| 6.95|\n",
            "|     I|13.0| 7.58|\n",
            "|     I| 9.0| 8.81|\n",
            "|     I|11.0| 8.33|\n",
            "|     I|14.0| 9.96|\n",
            "|     I| 6.0| 7.24|\n",
            "|     I| 4.0| 4.26|\n",
            "|     I|12.0|10.84|\n",
            "|     I| 7.0| 4.81|\n",
            "|     I| 5.0| 5.68|\n",
            "|    II|10.0| 9.14|\n",
            "|    II| 8.0| 8.14|\n",
            "|    II|13.0| 8.74|\n",
            "|    II| 9.0| 8.77|\n",
            "|    II|11.0| 9.26|\n",
            "|    II|14.0|  8.1|\n",
            "|    II| 6.0| 6.13|\n",
            "|    II| 4.0|  3.1|\n",
            "|    II|12.0| 9.13|\n",
            "+------+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import *\n",
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "df = spark.read.json(\"/content/sample_data/anscombe.json\", multiLine = True)\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending upon how your json data is presented you need to use the multiLine in option of json\n",
        "\n",
        "Now to read the multiple json file we can create list containing path of different files and then assign it into data section of .json"
      ],
      "metadata": {
        "id": "Ay_vfw1bLszj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DP4xHEUbLeHm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}